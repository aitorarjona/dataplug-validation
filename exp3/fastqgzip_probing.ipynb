{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import time\n",
    "import subprocess\n",
    "import os\n",
    "import dask\n",
    "import math\n",
    "import json\n",
    "import dask.bag as db\n",
    "from dask_cloudprovider.aws import EC2Cluster, FargateCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataplug import CloudObject\n",
    "from dataplug.formats.genomics.fastq import FASTQGZip, partition_reads_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dedup_fastq(slice):\n",
    "    print(\"Processing slice: \", slice)\n",
    "    task_start = time.time()\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    chunk_filename = tempfile.mktemp() + \".fastq\"\n",
    "    slice.to_file(chunk_filename)\n",
    "    t1 = time.perf_counter()\n",
    "\n",
    "    fetch_time = t1 - t0\n",
    "    chunk_size = os.stat(chunk_filename).st_size\n",
    "\n",
    "    filtered = chunk_filename + \".filtered\"\n",
    "    t0 = time.perf_counter()\n",
    "    proc = subprocess.check_output([\"fastq-filter\", \"-e\", \"0.001\", \"-o\", filtered, chunk_filename])\n",
    "    t1 = time.perf_counter()\n",
    "    filter_time = t1 - t0\n",
    "    print(proc)\n",
    "\n",
    "    deduped = filtered + \".dedup\"\n",
    "    t0 = time.perf_counter()\n",
    "    proc = subprocess.check_output([\"czid-dedup\", \"-i\", filtered, \"-o\", deduped])\n",
    "    t1 = time.perf_counter()\n",
    "    dedup_time = t1 - t0\n",
    "    print(proc)\n",
    "\n",
    "    res_size = os.stat(deduped).st_size\n",
    "\n",
    "    os.remove(chunk_filename)\n",
    "    os.remove(deduped)\n",
    "    os.remove(filtered)\n",
    "\n",
    "    task_end = time.time()\n",
    "    return {\n",
    "        \"chunk_size\": chunk_size,\n",
    "        \"result_size\": res_size,\n",
    "        \"fetch_time\": fetch_time,\n",
    "        \"filter_time\": filter_time,\n",
    "        \"dedup_time\": dedup_time,\n",
    "        \"task_time\": task_end - task_start,\n",
    "        \"task_start\": task_start,\n",
    "        \"task_end\": task_end,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_fastq(slice):\n",
    "    task_start = time.time()\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    chunk_filename = tempfile.mktemp() + \".fastq\"\n",
    "    slice.to_file(chunk_filename)\n",
    "    t1 = time.perf_counter()\n",
    "\n",
    "    fetch_time = t1 - t0\n",
    "    chunk_size = os.stat(chunk_filename).st_size\n",
    "\n",
    "    output_file = chunk_filename + \".fa\"\n",
    "    t0 = time.perf_counter()\n",
    "    proc = subprocess.check_output([\"seqtk\", \"seq\", \"-a\", chunk_filename, \"/dev/null\"])\n",
    "    t1 = time.perf_counter()\n",
    "    print(proc)\n",
    "\n",
    "    transform_time = t1 - t0\n",
    "\n",
    "    # output_file_2 = output_file + \".index\"\n",
    "    # t0 = time.perf_counter()\n",
    "    # proc = subprocess.check_output([\"seqtk\", \"trimfq\", \"-b\", \"5\", \"-e\", \"10\", output_file, \">\", output_file_2])\n",
    "    # t1 = time.perf_counter()\n",
    "    # trim_time = t1 - t0\n",
    "    # print(proc)\n",
    "\n",
    "    os.remove(chunk_filename)\n",
    "    os.remove(output_file)\n",
    "\n",
    "    task_end = time.time()\n",
    "    return {\n",
    "        \"chunk_size\": chunk_size,\n",
    "        \"fetch_time\": fetch_time,\n",
    "        \"transform_time\": transform_time,\n",
    "        # \"trim_time\": trim_time,\n",
    "        \"task_time\": task_end - task_start,\n",
    "        \"task_start\": task_start,\n",
    "        \"task_end\": task_end,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_config = {\n",
    "    \"aws_access_key_id\": \"\",\n",
    "    \"aws_secret_access_key\": \"\",\n",
    "    \"aws_session_token\": \"\",\n",
    "    \"region_name\": \"us-east-1\",\n",
    "    \"use_token\": False\n",
    "}\n",
    "\n",
    "co = CloudObject.from_path(FASTQGZip, \"s3://lithops-datasets/fastq.gz/13gb.fastq.gz\", storage_config=storage_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = EC2Cluster(\n",
    "    region=\"us-east-1\",\n",
    "    availability_zone=\"us-east-1a\",\n",
    "    subnet_id=\"subnet-0a95cf6e\",\n",
    "    worker_instance_type=\"m6i.2xlarge\",\n",
    "    scheduler_instance_type=\"m6i.large\",\n",
    "    docker_image=\"aitorarjona/dataplug-fastq-dask:0.4\",\n",
    "    security=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_slices = co.partition(partition_reads_batches, num_batches=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = cluster.get_client()\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(1)\n",
    "cluster.wait_for_workers(1)\n",
    "wl = db.from_sequence([data_slices[0]]).map(index_fastq)\n",
    "fut = client.compute(wl)\n",
    "fut.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_cpus = 8\n",
    "workers = math.ceil(len(data_slices) / vm_cpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.perf_counter()\n",
    "cluster.scale(workers)\n",
    "cluster.wait_for_workers(workers)\n",
    "t1 = time.perf_counter()\n",
    "\n",
    "print(f\"Scaling took {t1 - t0} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.perf_counter()\n",
    "# wl = db.from_sequence(data_slices).map(filter_dedup_fastq)\n",
    "wl = db.from_sequence(data_slices).map(index_fastq)\n",
    "fut = client.compute(wl)\n",
    "try:\n",
    "    results = fut.result()\n",
    "except Exception as e:\n",
    "    results = []\n",
    "    print(e)\n",
    "t1 = time.perf_counter()\n",
    "print(f\"Execution took {t1 - t0} seconds\")\n",
    "# results = fut.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment_name = f\"fastq-dedup-{len(data_slices)}b-{workers}w\"\n",
    "experiment_name = f\"fastq2fasta-{len(data_slices)}b-{workers}w\"\n",
    "with open(f\"{experiment_name}.json\", \"w\") as f:\n",
    "    f.write(json.dumps(results, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloud-data-validation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
